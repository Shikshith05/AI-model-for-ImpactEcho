{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmtFKZe0W4BgpR8f3HsoVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shikshith05/AI-model-for-ImpactEcho/blob/main/impactechomodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kyYDSjuWp5z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_impact_data(num_records=10000, fake_rate=0.1):\n",
        "    \"\"\"\n",
        "    Generate synthetic NGO impact verification data for ImpactEcho AI.\n",
        "\n",
        "    Args:\n",
        "        num_records (int): Number of total records to generate.\n",
        "        fake_rate (float): Fraction of fake or manipulated records.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Generated dataset with authenticity labels.\n",
        "    \"\"\"\n",
        "\n",
        "    num_fake = int(num_records * fake_rate)\n",
        "    num_real = num_records - num_fake\n",
        "\n",
        "    print(f\"Generating {num_records:,} NGO impact records ({num_real:,} real, {num_fake:,} fake)\")\n",
        "\n",
        "    ngo_ids = [f\"NGO-{i:03d}\" for i in range(1, 51)]\n",
        "    project_types = [\"Tree Plantation\", \"Education Drive\", \"Health Camp\", \"Clean Water\", \"Women Empowerment\", \"Food Distribution\"]\n",
        "    locations = [\"Delhi\", \"Bangalore\", \"Hyderabad\", \"Mumbai\", \"Kolkata\", \"Chennai\", \"Pune\"]\n",
        "\n",
        "    real_data, fake_data = [], []\n",
        "    base_time = datetime(2025, 9, 1, 10, 0, 0)\n",
        "\n",
        "    # ‚úÖ Generate real records\n",
        "    for _ in range(num_real):\n",
        "        ngo_id = random.choice(ngo_ids)\n",
        "        project = random.choice(project_types)\n",
        "        location = random.choice(locations)\n",
        "        timestamp = base_time + timedelta(days=random.randint(0, 60))\n",
        "\n",
        "        beneficiaries_reported = random.randint(80, 500)\n",
        "        verified_beneficiaries = beneficiaries_reported - random.randint(0, 10)\n",
        "        impact_score = round(random.uniform(0.8, 1.0), 2)\n",
        "        funding_amount = random.randint(50000, 300000)\n",
        "\n",
        "        media_link = f\"https://impactecho.org/media/{ngo_id}_{project.replace(' ', '_')}.jpg\"\n",
        "\n",
        "        real_data.append([\n",
        "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            ngo_id,\n",
        "            project,\n",
        "            location,\n",
        "            media_link,\n",
        "            beneficiaries_reported,\n",
        "            verified_beneficiaries,\n",
        "            funding_amount,\n",
        "            impact_score,\n",
        "            0  # is_fake\n",
        "        ])\n",
        "\n",
        "    # ‚ö†Ô∏è Generate fake records\n",
        "    for _ in range(num_fake):\n",
        "        ngo_id = random.choice(ngo_ids + [f\"FAKE-{i:03d}\" for i in range(51, 61)])\n",
        "        project = random.choice(project_types + [\"Ghost Project\", \"Phantom Relief\"])\n",
        "        location = random.choice(locations + [\"Unknown\", \"Remote Village\", \"N/A\"])\n",
        "        timestamp = base_time + timedelta(days=random.randint(0, 60))\n",
        "\n",
        "        # Fake or inconsistent metrics\n",
        "        beneficiaries_reported = random.randint(100, 1000)\n",
        "        verified_beneficiaries = max(0, beneficiaries_reported - random.randint(100, 900))\n",
        "        impact_score = round(random.uniform(0.0, 0.4), 2)\n",
        "        funding_amount = random.randint(100000, 400000)\n",
        "\n",
        "        # Broken or reused media\n",
        "        media_link = random.choice([\n",
        "            f\"https://impactecho.org/media/fake_{random.randint(1,999)}.jpg\",\n",
        "            \"https://imgur.com/fakeproof123\",\n",
        "            \"https://drive.google.com/brokenlink\"\n",
        "        ])\n",
        "\n",
        "        fake_data.append([\n",
        "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            ngo_id,\n",
        "            project,\n",
        "            location,\n",
        "            media_link,\n",
        "            beneficiaries_reported,\n",
        "            verified_beneficiaries,\n",
        "            funding_amount,\n",
        "            impact_score,\n",
        "            1  # is_fake\n",
        "        ])\n",
        "\n",
        "    # Combine + shuffle\n",
        "    df = pd.DataFrame(real_data + fake_data, columns=[\n",
        "        \"timestamp\", \"ngo_id\", \"project_type\", \"location\", \"media_link\",\n",
        "        \"reported_beneficiaries\", \"verified_beneficiaries\", \"funding_amount\",\n",
        "        \"impact_score\", \"is_fake\"\n",
        "    ])\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n‚úÖ Data generation complete!\")\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"Fake records: {df['is_fake'].sum():,}\")\n",
        "    print(f\"Fake rate: {df['is_fake'].mean() * 100:.2f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_to_json(df, filename=\"impact_data\"):\n",
        "    df.to_json(f\"{filename}.json\", orient=\"records\", indent=2)\n",
        "    df.to_json(f\"{filename}.jsonl\", orient=\"records\", lines=True)\n",
        "    print(f\"üíæ Saved to {filename}.json and {filename}.jsonl\")\n",
        "\n",
        "\n",
        "def display_sample(df, n=5):\n",
        "    print(f\"\\nüìä Showing {n} random samples:\\n\")\n",
        "    print(df.sample(n).to_string(index=False))\n",
        "    print(\"\\nüìà Summary:\\n\", df.describe())\n",
        "\n",
        "\n",
        "if __name__ == \"_main_\":\n",
        "    impact_df = generate_impact_data(num_records=5000, fake_rate=0.1)\n",
        "    display_sample(impact_df)\n",
        "    save_to_json(impact_df, \"impact_data_full\")\n",
        "\n",
        "    # Split for training/testing AI model\n",
        "    train_df = impact_df.sample(frac=0.8, random_state=42)\n",
        "    test_df = impact_df.drop(train_df.index)\n",
        "\n",
        "    save_to_json(train_df, \"impact_data_train\")\n",
        "    save_to_json(test_df, \"impact_data_test\")\n",
        "\n",
        "    print(\"\\nüìÇ Training records:\", len(train_df))\n",
        "    print(\"üìÇ Testing records:\", len(test_df))\n",
        "    print(\"\\n‚úÖ ImpactEcho synthetic dataset created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V2rn4IhJPCfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impact_df = generate_impact_data(num_records=5000, fake_rate=0.1)\n",
        "display_sample(impact_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRNVF_gSXRGU",
        "outputId": "4186fdd7-5663-4b33-90d8-75351ca75d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 5,000 NGO impact records (4,500 real, 500 fake)\n",
            "\n",
            "‚úÖ Data generation complete!\n",
            "Total records: 5,000\n",
            "Fake records: 500\n",
            "Fake rate: 10.00%\n",
            "\n",
            "üìä Showing 5 random samples:\n",
            "\n",
            "          timestamp  ngo_id      project_type  location                                                 media_link  reported_beneficiaries  verified_beneficiaries  funding_amount  impact_score  is_fake\n",
            "2025-09-14 10:00:00 NGO-025   Education Drive Bangalore   https://impactecho.org/media/NGO-025_Education_Drive.jpg                     457                     456          224677          0.80        0\n",
            "2025-10-31 10:00:00 NGO-027   Education Drive    Mumbai   https://impactecho.org/media/NGO-027_Education_Drive.jpg                     280                     276          109976          0.91        0\n",
            "2025-09-25 10:00:00 NGO-030       Health Camp Bangalore       https://impactecho.org/media/NGO-030_Health_Camp.jpg                     498                     491          150900          0.83        0\n",
            "2025-10-28 10:00:00 NGO-040       Health Camp   Chennai       https://impactecho.org/media/NGO-040_Health_Camp.jpg                     190                     180          209688          0.89        0\n",
            "2025-09-11 10:00:00 NGO-017 Women Empowerment    Mumbai https://impactecho.org/media/NGO-017_Women_Empowerment.jpg                     447                     447          283246          0.99        0\n",
            "\n",
            "üìà Summary:\n",
            "        reported_beneficiaries  verified_beneficiaries  funding_amount  \\\n",
            "count             5000.000000             5000.000000     5000.000000   \n",
            "mean               316.058800              273.283600   181956.077800   \n",
            "std                160.495922              139.897014    77390.927049   \n",
            "min                 80.000000                0.000000    50036.000000   \n",
            "25%                191.000000              159.000000   117718.500000   \n",
            "50%                306.000000              276.000000   180072.000000   \n",
            "75%                414.000000              388.250000   243890.750000   \n",
            "max                998.000000              888.000000   398741.000000   \n",
            "\n",
            "       impact_score     is_fake  \n",
            "count   5000.000000  5000.00000  \n",
            "mean       0.829834     0.10000  \n",
            "std        0.218861     0.30003  \n",
            "min        0.000000     0.00000  \n",
            "25%        0.830000     0.00000  \n",
            "50%        0.890000     0.00000  \n",
            "75%        0.940000     0.00000  \n",
            "max        1.000000     1.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy dataframe to avoid modifying original\n",
        "df = impact_df.copy()\n",
        "\n",
        "# 1Ô∏è‚É£ Derived feature\n",
        "df['beneficiary_gap'] = df['reported_beneficiaries'] - df['verified_beneficiaries']\n",
        "\n",
        "# 2Ô∏è‚É£ Features & labels\n",
        "features = [\n",
        "    'reported_beneficiaries',\n",
        "    'verified_beneficiaries',\n",
        "    'beneficiary_gap',\n",
        "    'funding_amount',\n",
        "    'impact_score',\n",
        "    'ngo_id',\n",
        "    'project_type',\n",
        "    'location'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['is_fake']\n",
        "\n",
        "# 3Ô∏è‚É£ Encode categorical columns\n",
        "categorical_cols = ['ngo_id', 'project_type', 'location']\n",
        "encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "# Check preprocessed data\n",
        "print(\"Preprocessed feature sample:\")\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJbDre1wYdam",
        "outputId": "0804a220-d98f-4c73-8ecc-baadd1e3c5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed feature sample:\n",
            "   reported_beneficiaries  verified_beneficiaries  beneficiary_gap  \\\n",
            "0                     349                     345                4   \n",
            "1                     192                     185                7   \n",
            "2                     314                     311                3   \n",
            "3                     124                     123                1   \n",
            "4                     469                     467                2   \n",
            "\n",
            "   funding_amount  impact_score  ngo_id  project_type  location  \n",
            "0          273330          0.90      22             2         2  \n",
            "1           92541          0.88      24             4         0  \n",
            "2           62826          0.96      38             6         0  \n",
            "3          104507          0.92      39             2         5  \n",
            "4           79740          0.96      56             2         3  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n",
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n",
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1Ô∏è‚É£ Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2Ô∏è‚É£ Normalize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3Ô∏è‚É£ Build simple NN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output: probability of being fake\n",
        "])\n",
        "\n",
        "# 4Ô∏è‚É£ Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5Ô∏è‚É£ Train model\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# 6Ô∏è‚É£ Evaluate\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_siEwi8sXRIe",
        "outputId": "2240fe85-a81a-4475-b0a9-70d761717aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8523 - loss: 0.5082 - val_accuracy: 0.9875 - val_loss: 0.1043\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9880 - loss: 0.0759 - val_accuracy: 1.0000 - val_loss: 0.0186\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0167 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 8.7149e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 6.4581e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.8271e-04 - val_accuracy: 1.0000 - val_loss: 4.9553e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.7152e-04 - val_accuracy: 1.0000 - val_loss: 3.9186e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.2917e-04 - val_accuracy: 1.0000 - val_loss: 3.1654e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.1821e-04 - val_accuracy: 1.0000 - val_loss: 2.6088e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.0804e-04 - val_accuracy: 1.0000 - val_loss: 2.1726e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.1204e-04 - val_accuracy: 1.0000 - val_loss: 1.8396e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.4771e-04 - val_accuracy: 1.0000 - val_loss: 1.5706e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.0219e-04 - val_accuracy: 1.0000 - val_loss: 1.3568e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.1281e-04 - val_accuracy: 1.0000 - val_loss: 1.1753e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.3991e-04 - val_accuracy: 1.0000 - val_loss: 1.0310e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.4639e-04 - val_accuracy: 1.0000 - val_loss: 9.0916e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1728e-04 - val_accuracy: 1.0000 - val_loss: 8.0476e-05\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 9.3295e-05\n",
            "\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example new record\n",
        "new_record = {\n",
        "    'reported_beneficiaries': 300,\n",
        "    'verified_beneficiaries': 290,\n",
        "    'beneficiary_gap': 10,\n",
        "    'funding_amount': 150000,\n",
        "    'impact_score': 0.95,\n",
        "    'ngo_id': 'NGO-001',\n",
        "    'project_type': 'Tree Plantation',\n",
        "    'location': 'Delhi'\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "new_df = pd.DataFrame([new_record])\n",
        "\n",
        "# Encode categorical features using the same LabelEncoders\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    new_df[col] = encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numeric features using the same scaler\n",
        "new_scaled = scaler.transform(new_df)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_scaled)\n",
        "prob = model.predict_proba(new_scaled)[0][0] if hasattr(model, \"predict_proba\") else prediction[0]\n",
        "\n",
        "print(\"Predicted class (0=real, 1=fake):\", int(prediction[0] > 0.5))\n",
        "print(\"Probability of being fake:\", float(prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_-W92VXRKf",
        "outputId": "bad37a99-16f2-4e01-ee64-e82adc7362c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Predicted class (0=real, 1=fake): 0\n",
            "Probability of being fake: 9.480218068347313e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3612469322.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Predicted class (0=real, 1=fake):\", int(prediction[0] > 0.5))\n",
            "/tmp/ipython-input-3612469322.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Probability of being fake:\", float(prob))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example real NGO record\n",
        "real_record = {\n",
        "    'reported_beneficiaries': 250,\n",
        "    'verified_beneficiaries': 245,\n",
        "    'beneficiary_gap': 5,\n",
        "    'funding_amount': 120000,\n",
        "    'impact_score': 0.92,\n",
        "    'ngo_id': 'NGO-007',\n",
        "    'project_type': 'Education Drive',\n",
        "    'location': 'Bangalore'\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "new_df = pd.DataFrame([real_record])\n",
        "\n",
        "# Encode categorical features using the same LabelEncoders\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    new_df[col] = encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numeric features\n",
        "new_scaled = scaler.transform(new_df)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_scaled)\n",
        "prob = prediction[0][0]  # TensorFlow outputs an array\n",
        "\n",
        "print(\"Predicted class (0=real, 1=fake):\", int(prediction[0][0] > 0.5))\n",
        "print(\"Probability of being fake:\", float(prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow_-MmYUXRMy",
        "outputId": "f0979236-d9a1-4af9-ca27-37df49d8a8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Predicted class (0=real, 1=fake): 0\n",
            "Probability of being fake: 8.81987034517806e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_records = [\n",
        "    # huge gap, low impact score, high funding\n",
        "    {\n",
        "        'reported_beneficiaries': 900,\n",
        "        'verified_beneficiaries': 50,\n",
        "        'beneficiary_gap': 850,\n",
        "        'funding_amount': 350000,\n",
        "        'impact_score': 0.05,\n",
        "        'ngo_id': 'NGO-012',\n",
        "        'project_type': 'Food Distribution',\n",
        "        'location': 'Mumbai'\n",
        "    },\n",
        "    # reported high but verified zero, tiny impact score\n",
        "    {\n",
        "        'reported_beneficiaries': 700,\n",
        "        'verified_beneficiaries': 0,\n",
        "        'beneficiary_gap': 700,\n",
        "        'funding_amount': 300000,\n",
        "        'impact_score': 0.02,\n",
        "        'ngo_id': 'NGO-021',\n",
        "        'project_type': 'Health Camp',\n",
        "        'location': 'Delhi'\n",
        "    },\n",
        "    # moderate reported but verified very low, suspicious funding\n",
        "    {\n",
        "        'reported_beneficiaries': 450,\n",
        "        'verified_beneficiaries': 20,\n",
        "        'beneficiary_gap': 430,\n",
        "        'funding_amount': 280000,\n",
        "        'impact_score': 0.10,\n",
        "        'ngo_id': 'NGO-033',\n",
        "        'project_type': 'Education Drive',\n",
        "        'location': 'Kolkata'\n",
        "    },\n",
        "    # extreme gap + low score (looks fabricated)\n",
        "    {\n",
        "        'reported_beneficiaries': 1000,\n",
        "        'verified_beneficiaries': 10,\n",
        "        'beneficiary_gap': 990,\n",
        "        'funding_amount': 400000,\n",
        "        'impact_score': 0.01,\n",
        "        'ngo_id': 'NGO-005',\n",
        "        'project_type': 'Tree Plantation',\n",
        "        'location': 'Chennai'\n",
        "    },\n",
        "    # suspicious combination: medium reported but very low verified and low score\n",
        "    {\n",
        "        'reported_beneficiaries': 320,\n",
        "        'verified_beneficiaries': 5,\n",
        "        'beneficiary_gap': 315,\n",
        "        'funding_amount': 200000,\n",
        "        'impact_score': 0.08,\n",
        "        'ngo_id': 'NGO-018',\n",
        "        'project_type': 'Women Empowerment',\n",
        "        'location': 'Pune'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "fake_df = pd.DataFrame(fake_records)\n",
        "\n",
        "# Encode categorical columns using the same LabelEncoders used during training\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    # If the encoder doesn't know a label, map to a fallback (most common label)\n",
        "    le = encoders[col]\n",
        "    def safe_transform(val):\n",
        "        if val in le.classes_:\n",
        "            return le.transform([val])[0]\n",
        "        else:\n",
        "            # fallback: use index 0 class (or you can choose another strategy)\n",
        "            return 0\n",
        "    fake_df[col] = fake_df[col].apply(safe_transform)\n",
        "\n",
        "# Scale numeric features using the same scaler\n",
        "fake_scaled = scaler.transform(fake_df)\n",
        "\n",
        "# Predict with the trained TensorFlow model\n",
        "preds = model.predict(fake_scaled)          # sigmoid outputs between 0 and 1\n",
        "pred_labels = (preds > 0.5).astype(int).ravel()\n",
        "\n",
        "# Show results\n",
        "fake_df['predicted_prob_fake'] = preds.ravel()\n",
        "fake_df['predicted_label'] = np.where(pred_labels==1, 'FAKE', 'REAL')\n",
        "\n",
        "print(\"\\nPredictions for fake-looking records:\\n\")\n",
        "print(fake_df[['reported_beneficiaries','verified_beneficiaries','beneficiary_gap',\n",
        "               'funding_amount','impact_score','ngo_id','project_type','location',\n",
        "               'predicted_prob_fake','predicted_label']])"
      ],
      "metadata": {
        "id": "kYKHiv_QXRPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebbd436-a71c-48c0-8d51-a4f3113ca33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "\n",
            "Predictions for fake-looking records:\n",
            "\n",
            "   reported_beneficiaries  verified_beneficiaries  beneficiary_gap  \\\n",
            "0                     900                      50              850   \n",
            "1                     700                       0              700   \n",
            "2                     450                      20              430   \n",
            "3                    1000                      10              990   \n",
            "4                     320                       5              315   \n",
            "\n",
            "   funding_amount  impact_score  ngo_id  project_type  location  \\\n",
            "0          350000          0.05      21             2         5   \n",
            "1          300000          0.02      30             4         2   \n",
            "2          280000          0.10      42             1         4   \n",
            "3          400000          0.01      14             6         1   \n",
            "4          200000          0.08      27             7         7   \n",
            "\n",
            "   predicted_prob_fake predicted_label  \n",
            "0             1.000000            FAKE  \n",
            "1             1.000000            FAKE  \n",
            "2             0.999993            FAKE  \n",
            "3             1.000000            FAKE  \n",
            "4             0.999893            FAKE  \n"
          ]
        }
      ]
    }
  ]
}